{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Work, Part 4 - Machine Learning\n",
    "## 1. Introduction\n",
    "This project involves analysing data with implementing machine learning model in a Jupyter Notebook and creating a multi-page online app with Streamlit, with all work and code shared on GitHub. AI tools (e.g., ChatGPT) were utilized during the project to clarify requirements and to gain a deeper understanding of the technologies used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Repository and App Links\n",
    "- GitHub: https://github.com/Indraadhikari/IND320_Indra\n",
    "- Streamlit app: https://ind320-k2r8aymxk9takanegm8e3y.streamlit.app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Project Overview\n",
    "### 3.1 AI Usage Description\n",
    "In this part of the project, I leveraged AI (ChatGPT/Claude) as a development assistant. AI helped in designing a multi-page Streamlit application, debugging Python issues with time series and geospatial data, and improving code structure. Specifically, AI assisted in:\n",
    "\n",
    "- Handling misaligned DataFrames and cleaning missing values in energy and meteorological data to avoid SARIMAX errors\n",
    "- Implementing GeoJSON processing for interactive maps with coordinate selection and session state management\n",
    "- Translating Tabler (2003) snow drift calculations and creating 16-sector wind rose diagrams\n",
    "- Structuring sliding window correlation analysis with configurable lag parameters\n",
    "- Debugging statsmodels errors related to exogenous variables and timestamp arithmetic\n",
    "- Implementing caching strategies and Plotly visualizations with confidence intervals\n",
    "\n",
    "All AI outputs were validated and adjusted to ensure correctness and reproducibility.\n",
    "\n",
    "### 3.2 Project Log\n",
    "Updated the global selection mechanism using `st.session_state` to store selected price area (NO1-NO5) and coordinates, ensuring consistency across all pages.\n",
    "\n",
    "**Interactive Map:** Created GeoJSON-based choropleth visualization of Norwegian price areas. Users can click to select areas and coordinates, with mean energy production calculated by time interval (7-90 days) and energy group (hydro, wind, solar, etc.). Selected areas are highlighted with red outlines and coordinates marked with stars.\n",
    "\n",
    "**Snow Drift Analysis:** Implemented Tabler (2003) method with seasons defined as July 1 to June 30. Fetched hourly meteorological data from Open-Meteo API (temperature, precipitation, wind speed, direction). Calculated potential and actual snow transport (Qupot, Qt) and created wind rose diagrams showing directional distribution. Uses coordinates from map selection.\n",
    "\n",
    "**Correlation Analysis:** Built sliding window correlation with selectable lag (-168 to +168 hours) between meteorological variables and energy production. Aligned hourly weather data with energy time series, detected extreme events (>2σ), and created three-panel visualizations showing correlation changes, weather patterns, and production levels over time.\n",
    "\n",
    "**SARIMAX Forecasting:** Implemented time series forecasting with full parameter control (p,d,q)(P,D,Q,s). Users select energy group, training period, and forecast horizon (24-720 hours). Optional exogenous variables use hourly seasonal patterns for future periods. Results display historical data, forecasts, and 95% confidence intervals with model metrics (AIC, BIC).\n",
    "\n",
    "**Data Pipeline:** Elhub API → MongoDB → Streamlit (energy data); Open-Meteo API → cleaning/alignment → analysis (weather data); NVE API → GeoPandas → Plotly (GeoJSON).\n",
    "\n",
    "The completed workflow demonstrates a full data pipeline: acquiring data dynamically via APIs, performing time‑series analysis, detecting anomalies, and presenting interactive results through a structured Streamlit interface and Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import pandas as pd\n",
    "import calendar\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fftpack import dct, idct\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from scipy.signal import spectrogram\n",
    "from cassandra.cluster import Cluster\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Extraction and Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Connection Check for Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to Cassandra!\n",
      "Cluster name: Test Cluster\n",
      "Hosts: [<Host: 127.0.0.1:9042 datacenter1>]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    cluster = Cluster(['localhost'], port=9042)\n",
    "    session = cluster.connect()\n",
    "    print(\"✅ Connected to Cassandra!\")\n",
    "    print(\"Cluster name:\", cluster.metadata.cluster_name)\n",
    "    print(\"Hosts:\", cluster.metadata.all_hosts())\n",
    "    cluster.shutdown()\n",
    "except Exception as e:\n",
    "    print(\"❌ Connection failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Connection Check for MangoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "c_file = '/Users/indra/Documents/Masters in Data Science/Data to Decision/IND320_Indra/No_sync/MongoDB.txt' #creadential file\n",
    "USR, PWD = open(c_file).read().splitlines()\n",
    "\n",
    "uri = \"mongodb+srv://\"+USR+\":\"+PWD+\"@cluster0.wmoqhtp.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "\n",
    "# Create a new client and connect to the server\n",
    "client = MongoClient(uri)\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Reading Data from  Elhub API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reading_data_from_elhub_api(entity,dataset,year=[2021]): \n",
    "\n",
    "    headers = {           \n",
    "    }\n",
    "\n",
    "    endpoint = \"https://api.elhub.no/energy-data/v0/\"\n",
    "    entity = entity # 'price-areas'\n",
    "    dataset = dataset # \"PRODUCTION_PER_GROUP_MBA_HOUR\"\n",
    "    #startdate = '2022-01-01T00:20:00%2B02:00'\n",
    "    #enddate = '2024-12-31T23:59:59%2B02:00'\n",
    "    year = year # [2022, 2023, 2024]\n",
    "\n",
    "    datafield = dataset.lower().split(\"_\")\n",
    "    camel_style_dataset = datafield[0] + \"\".join(word.capitalize() for word in datafield[1:])\n",
    "\n",
    "    dates = []\n",
    "    for i in year:\n",
    "        year = i\n",
    "        # accessing the data for a month at a time as Endpoint does not allow us to get for a whole year.\n",
    "        for month in range(1, 13):\n",
    "            # Get number of days in month\n",
    "            _, last_day = calendar.monthrange(year, month)\n",
    "            \n",
    "            # Format month and day properly (e.g. 01, 02, …)\n",
    "            startdate = f\"{year}-{month:02d}-01T00:20:00%2B02:00\"\n",
    "            enddate = f\"{year}-{month:02d}-{last_day:02d}T23:59:59%2B02:00\"\n",
    "            \n",
    "            dates.append((startdate, enddate))\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    for startdate, enddate in dates:\n",
    "        #print(f\"Start: {start}   End: {end}\")\n",
    "        data = []\n",
    "        response = requests.get(f\"{endpoint}{entity}?dataset={dataset}&startDate={startdate}&endDate={enddate}\", headers=headers)\n",
    "        #print(response.status_code)\n",
    "        data = response.json()\n",
    "        #data['data'][1]['attributes']['productionPerGroupMbaHour']\n",
    "        datafield = dataset.replace(\"_\",\"\")\n",
    "        for i in data['data']:\n",
    "            all_data.extend(i['attributes'][camel_style_dataset])\n",
    "    df = pd.DataFrame(all_data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1 Reading PRODUCTION_PER_GROUP_MBA_HOUR Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Shape of the Production dataframe: (656700, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endTime</th>\n",
       "      <th>lastUpdatedTime</th>\n",
       "      <th>priceArea</th>\n",
       "      <th>productionGroup</th>\n",
       "      <th>quantityKwh</th>\n",
       "      <th>startTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-01T02:00:00+01:00</td>\n",
       "      <td>2025-02-01T18:02:57+01:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>1246209.4</td>\n",
       "      <td>2022-01-01T01:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-01T03:00:00+01:00</td>\n",
       "      <td>2025-02-01T18:02:57+01:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>1271757.0</td>\n",
       "      <td>2022-01-01T02:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-01T04:00:00+01:00</td>\n",
       "      <td>2025-02-01T18:02:57+01:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>1204251.8</td>\n",
       "      <td>2022-01-01T03:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-01T05:00:00+01:00</td>\n",
       "      <td>2025-02-01T18:02:57+01:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>1202086.9</td>\n",
       "      <td>2022-01-01T04:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-01T06:00:00+01:00</td>\n",
       "      <td>2025-02-01T18:02:57+01:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>1235809.9</td>\n",
       "      <td>2022-01-01T05:00:00+01:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     endTime            lastUpdatedTime priceArea  \\\n",
       "0  2022-01-01T02:00:00+01:00  2025-02-01T18:02:57+01:00       NO1   \n",
       "1  2022-01-01T03:00:00+01:00  2025-02-01T18:02:57+01:00       NO1   \n",
       "2  2022-01-01T04:00:00+01:00  2025-02-01T18:02:57+01:00       NO1   \n",
       "3  2022-01-01T05:00:00+01:00  2025-02-01T18:02:57+01:00       NO1   \n",
       "4  2022-01-01T06:00:00+01:00  2025-02-01T18:02:57+01:00       NO1   \n",
       "\n",
       "  productionGroup  quantityKwh                  startTime  \n",
       "0           hydro    1246209.4  2022-01-01T01:00:00+01:00  \n",
       "1           hydro    1271757.0  2022-01-01T02:00:00+01:00  \n",
       "2           hydro    1204251.8  2022-01-01T03:00:00+01:00  \n",
       "3           hydro    1202086.9  2022-01-01T04:00:00+01:00  \n",
       "4           hydro    1235809.9  2022-01-01T05:00:00+01:00  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity = 'price-areas'\n",
    "dataset = \"PRODUCTION_PER_GROUP_MBA_HOUR\"\n",
    "year = [2021, 2022, 2023, 2024]\n",
    "\n",
    "df = reading_data_from_elhub_api(entity=entity,dataset=dataset,year=year)\n",
    "print(f\"The Shape of the Production dataframe: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2 Reading CONSUMPTION_PER_GROUP_MBA_HOUR Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Shape of the Consumption dataframe: (875400, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>consumptionGroup</th>\n",
       "      <th>endTime</th>\n",
       "      <th>lastUpdatedTime</th>\n",
       "      <th>meteringPointCount</th>\n",
       "      <th>priceArea</th>\n",
       "      <th>quantityKwh</th>\n",
       "      <th>startTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cabin</td>\n",
       "      <td>2021-01-01T02:00:00+01:00</td>\n",
       "      <td>2024-12-20T10:35:40+01:00</td>\n",
       "      <td>100607</td>\n",
       "      <td>NO1</td>\n",
       "      <td>171335.12</td>\n",
       "      <td>2021-01-01T01:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cabin</td>\n",
       "      <td>2021-01-01T03:00:00+01:00</td>\n",
       "      <td>2024-12-20T10:35:40+01:00</td>\n",
       "      <td>100607</td>\n",
       "      <td>NO1</td>\n",
       "      <td>164912.02</td>\n",
       "      <td>2021-01-01T02:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cabin</td>\n",
       "      <td>2021-01-01T04:00:00+01:00</td>\n",
       "      <td>2024-12-20T10:35:40+01:00</td>\n",
       "      <td>100607</td>\n",
       "      <td>NO1</td>\n",
       "      <td>160265.77</td>\n",
       "      <td>2021-01-01T03:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cabin</td>\n",
       "      <td>2021-01-01T05:00:00+01:00</td>\n",
       "      <td>2024-12-20T10:35:40+01:00</td>\n",
       "      <td>100607</td>\n",
       "      <td>NO1</td>\n",
       "      <td>159828.69</td>\n",
       "      <td>2021-01-01T04:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cabin</td>\n",
       "      <td>2021-01-01T06:00:00+01:00</td>\n",
       "      <td>2024-12-20T10:35:40+01:00</td>\n",
       "      <td>100607</td>\n",
       "      <td>NO1</td>\n",
       "      <td>160388.17</td>\n",
       "      <td>2021-01-01T05:00:00+01:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  consumptionGroup                    endTime            lastUpdatedTime  \\\n",
       "0            cabin  2021-01-01T02:00:00+01:00  2024-12-20T10:35:40+01:00   \n",
       "1            cabin  2021-01-01T03:00:00+01:00  2024-12-20T10:35:40+01:00   \n",
       "2            cabin  2021-01-01T04:00:00+01:00  2024-12-20T10:35:40+01:00   \n",
       "3            cabin  2021-01-01T05:00:00+01:00  2024-12-20T10:35:40+01:00   \n",
       "4            cabin  2021-01-01T06:00:00+01:00  2024-12-20T10:35:40+01:00   \n",
       "\n",
       "   meteringPointCount priceArea  quantityKwh                  startTime  \n",
       "0              100607       NO1    171335.12  2021-01-01T01:00:00+01:00  \n",
       "1              100607       NO1    164912.02  2021-01-01T02:00:00+01:00  \n",
       "2              100607       NO1    160265.77  2021-01-01T03:00:00+01:00  \n",
       "3              100607       NO1    159828.69  2021-01-01T04:00:00+01:00  \n",
       "4              100607       NO1    160388.17  2021-01-01T05:00:00+01:00  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity = 'price-areas'\n",
    "dataset = \"CONSUMPTION_PER_GROUP_MBA_HOUR\"\n",
    "year = [2021, 2022, 2023, 2024]\n",
    "\n",
    "df_consumption = reading_data_from_elhub_api(entity=entity,dataset=dataset,year=year)\n",
    "print(f\"The Shape of the Consumption dataframe: {df_consumption.shape}\")\n",
    "df_consumption.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Creating Keyspace and Table in Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:cassandra.cluster:Cluster.__init__ called with contact_points specified, but no load_balancing_policy. In the next major version, this will raise an error; please specify a load-balancing policy. (contact_points = ['localhost'], lbp = None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for ::1:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
      "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for ::1:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x17b338b90>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#----------- Production Data Table --------------\n",
    "#starting cassandra conection session\n",
    "cluster = Cluster(['localhost'], port=9042)\n",
    "session = cluster.connect()\n",
    "\n",
    "#making id columns for Premary Key for the table.\n",
    "if \"id\" not in df.columns:\n",
    "    df = df.reset_index().rename(columns={'index': 'id'})\n",
    "else:\n",
    "    pass\n",
    "df.columns\n",
    "\n",
    "columns = \", \".join([f\"{col} text\" for col in df.columns]) # type is text\n",
    "primary_key = df.columns[0]  # first column as primary key (id; index of the df)\n",
    "\n",
    "# Create a keyspace (database)\n",
    "session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS infindra\n",
    "    WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};\n",
    "\"\"\")\n",
    "\n",
    "#ALTER KEYSPACE infindra WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};\n",
    "\n",
    "session.set_keyspace('infindra')\n",
    "\n",
    "#Creating Tables\n",
    "create_query = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS production_per_group (\n",
    "    {columns},\n",
    "    PRIMARY KEY ({primary_key})\n",
    ")\n",
    "\"\"\"\n",
    "session.execute(create_query)\n",
    "#session.execute(\"TRUNCATE TABLE production_per_group;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x15c913510>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#----------- Consumption Data Table --------------\n",
    "cluster = Cluster(['localhost'], port=9042)\n",
    "session = cluster.connect()\n",
    "\n",
    "#making id columns for Premary Key for the table.\n",
    "if \"id\" not in df_consumption.columns:\n",
    "    df_consumption = df_consumption.reset_index().rename(columns={'index': 'id'})\n",
    "else:\n",
    "    pass\n",
    "df_consumption.columns\n",
    "\n",
    "columns = \", \".join([f\"{col} text\" for col in df_consumption.columns]) # type is text\n",
    "primary_key = df_consumption.columns[0]  # first column as primary key (id; index of the df)\n",
    "\n",
    "# Create a keyspace (database)\n",
    "session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS infindra\n",
    "    WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};\n",
    "\"\"\")\n",
    "\n",
    "#ALTER KEYSPACE infindra WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};\n",
    "\n",
    "session.set_keyspace('infindra')\n",
    "\n",
    "#Creating Tables\n",
    "create_query = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS consumption_per_group (\n",
    "    {columns},\n",
    "    PRIMARY KEY ({primary_key})\n",
    ")\n",
    "\"\"\"\n",
    "session.execute(create_query)\n",
    "#session.execute(\"TRUNCATE TABLE consumption_per_group;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Inserting data to Cassandra using Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark version: 3.5.1\n"
     ]
    }
   ],
   "source": [
    "os.environ['JAVA_HOME'] = \"/Library/Java/JavaVirtualMachines/microsoft-17.jdk/Contents/Home\"\n",
    "os.environ['PATH'] = os.path.join(os.environ['JAVA_HOME'], 'bin') + \":\" + os.environ['PATH']\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"CassandraReadTest\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.jars.repositories\",\n",
    "            \"https://repos.spark-packages.org,https://oss.sonatype.org/content/repositories/releases/\")\n",
    "    .config(\"spark.jars.packages\",\n",
    "            \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.1,\"\n",
    "            \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.2\")\n",
    "    .config(\"spark.cassandra.connection.host\", \"localhost\")\n",
    "    .config(\"spark.cassandra.connection.port\", \"9042\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(f\"✅ Spark version: {spark.version}\")\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/03 19:19:20 WARN TaskSetManager: Stage 0 contains a task of very large size (8830 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/12/03 19:19:25 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 0 (TID 0): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+---------------+-----------+--------------------+\n",
      "|             endtime|     lastupdatedtime|pricearea|productiongroup|quantitykwh|           starttime|\n",
      "+--------------------+--------------------+---------+---------------+-----------+--------------------+\n",
      "|2022-01-01T02:00:...|2025-02-01T18:02:...|      NO1|          hydro|  1246209.4|2022-01-01T01:00:...|\n",
      "|2022-01-01T03:00:...|2025-02-01T18:02:...|      NO1|          hydro|  1271757.0|2022-01-01T02:00:...|\n",
      "+--------------------+--------------------+---------+---------------+-----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/03 19:19:27 WARN TaskSetManager: Stage 1 contains a task of very large size (12746 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+--------------------+--------------------+------------------+---------+-----------+--------------------+\n",
      "| id|consumptiongroup|             endtime|     lastupdatedtime|meteringpointcount|pricearea|quantitykwh|           starttime|\n",
      "+---+----------------+--------------------+--------------------+------------------+---------+-----------+--------------------+\n",
      "|  0|           cabin|2021-01-01T02:00:...|2024-12-20T10:35:...|            100607|      NO1|  171335.12|2021-01-01T01:00:...|\n",
      "|  1|           cabin|2021-01-01T03:00:...|2024-12-20T10:35:...|            100607|      NO1|  164912.02|2021-01-01T02:00:...|\n",
      "+---+----------------+--------------------+--------------------+------------------+---------+-----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/03 19:19:31 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 1 (TID 1): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(df)\n",
    "spark_df_consumption = spark.createDataFrame(df_consumption)\n",
    "\n",
    "spark_df = spark_df.toDF(*[c.lower() for c in spark_df.columns]) #changing the colomns name in lower case to match casandra table\n",
    "spark_df_consumption = spark_df_consumption.toDF(*[c.lower() for c in spark_df_consumption.columns])\n",
    "#spark_df.printSchema()\n",
    "# Show the data\n",
    "print(spark_df.show(2))\n",
    "print(spark_df_consumption.show(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/03 19:20:57 WARN DeprecatedConfigParameter: spark.cassandra.output.throughput_mb_per_sec is deprecated (DSE 6.0.0) and has been automatically replaced with parameter spark.cassandra.output.throughputMBPerSec. \n",
      "25/12/03 19:20:58 WARN DeprecatedConfigParameter: spark.cassandra.output.throughput_mb_per_sec is deprecated (DSE 6.0.0) and has been automatically replaced with parameter spark.cassandra.output.throughputMBPerSec. \n",
      "25/12/03 19:20:58 WARN DeprecatedConfigParameter: spark.cassandra.output.throughput_mb_per_sec is deprecated (DSE 6.0.0) and has been automatically replaced with parameter spark.cassandra.output.throughputMBPerSec. \n",
      "25/12/03 19:20:59 WARN TaskSetManager: Stage 2 contains a task of very large size (12746 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to Cassandra!\n"
     ]
    }
   ],
   "source": [
    "# Optimize Cassandra write settings\n",
    "spark.conf.set(\"spark.cassandra.output.concurrent.writes\", \"5\")\n",
    "spark.conf.set(\"spark.cassandra.output.throughput_mb_per_sec\", \"200\")\n",
    "spark.conf.set(\"spark.cassandra.output.batch.size.rows\", \"1000\")\n",
    "\n",
    "# Write DataFrame to Cassandra\n",
    "# keyspace='infindra' and table='production_per_group' exist in Cassandra\n",
    "spark_df.write \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .option(\"keyspace\", \"infindra\") \\\n",
    "    .option(\"table\", \"production_per_group\") \\\n",
    "    .option(\"confirm.truncate\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "# keyspace='infindra' and table='consumption_per_group' exist in Cassandra\n",
    "spark_df_consumption.write \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .option(\"keyspace\", \"infindra\") \\\n",
    "    .option(\"table\", \"consumption_per_group\") \\\n",
    "    .option(\"confirm.truncate\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"Data successfully written to Cassandra!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Reading Data from Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe for Elhub API´s PRODUCTION_PER_GROUP_MBA_HOUR data\n",
    "def read_from_cassandra(table = 'production_per_group'):\n",
    "    df = spark.read \\\n",
    "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "        .option(\"keyspace\", \"infindra\") \\\n",
    "        .option(\"table\", table) \\\n",
    "        .load()\n",
    "    return df\n",
    "\n",
    "def save_as_csv(df,file_path):\n",
    "    df.coalesce(1) \\\n",
    "    .write \\\n",
    "    .option(\"header\", True) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(priceArea='NO2', productionGroup='solar', startTime='2024-12-26T19:00:00+01:00', quantityKwh='72.243'),\n",
       " Row(priceArea='NO4', productionGroup='hydro', startTime='2023-04-25T09:00:00+02:00', quantityKwh='3419812.2'),\n",
       " Row(priceArea='NO4', productionGroup='wind', startTime='2024-07-29T15:00:00+02:00', quantityKwh='94313.76'),\n",
       " Row(priceArea='NO3', productionGroup='hydro', startTime='2023-02-03T11:00:00+01:00', quantityKwh='2922081.5'),\n",
       " Row(priceArea='NO2', productionGroup='thermal', startTime='2022-06-14T00:00:00+02:00', quantityKwh='24708.13')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_p = read_from_cassandra(table = 'production_per_group')\n",
    "save_as_csv(df_p, file_path= \"No_sync/P_Energy\")\n",
    "# selected_df.show()\n",
    "df_p.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/03 20:12:46 WARN DeprecatedConfigParameter: spark.cassandra.output.throughput_mb_per_sec is deprecated (DSE 6.0.0) and has been automatically replaced with parameter spark.cassandra.output.throughputMBPerSec. \n",
      "25/12/03 20:12:46 WARN DeprecatedConfigParameter: spark.cassandra.output.throughput_mb_per_sec is deprecated (DSE 6.0.0) and has been automatically replaced with parameter spark.cassandra.output.throughputMBPerSec. \n",
      "25/12/03 20:12:46 WARN DeprecatedConfigParameter: spark.cassandra.output.throughput_mb_per_sec is deprecated (DSE 6.0.0) and has been automatically replaced with parameter spark.cassandra.output.throughputMBPerSec. \n",
      "25/12/03 20:13:10 WARN DeprecatedConfigParameter: spark.cassandra.output.throughput_mb_per_sec is deprecated (DSE 6.0.0) and has been automatically replaced with parameter spark.cassandra.output.throughputMBPerSec. \n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id='781983', consumptiongroup='primary', endtime='2024-07-22T10:00:00+02:00', lastupdatedtime='2025-11-25T19:23:21+01:00', meteringpointcount='2039', pricearea='NO5', quantitykwh='12853.232', starttime='2024-07-22T09:00:00+02:00'),\n",
       " Row(id='718143', consumptiongroup='cabin', endtime='2024-04-15T19:00:00+02:00', lastupdatedtime='2025-11-24T21:02:14+01:00', meteringpointcount='62483', pricearea='NO3', quantitykwh='31051.992', starttime='2024-04-15T18:00:00+02:00'),\n",
       " Row(id='474836', consumptiongroup='primary', endtime='2023-03-26T05:00:00+02:00', lastupdatedtime='2025-11-25T06:28:28+01:00', meteringpointcount='8092', pricearea='NO1', quantitykwh='63533.51', starttime='2023-03-26T04:00:00+02:00'),\n",
       " Row(id='179217', consumptiongroup='household', endtime='2021-10-02T21:00:00+02:00', lastupdatedtime='2024-12-20T10:35:40+01:00', meteringpointcount='248664', pricearea='NO5', quantitykwh='391079.78', starttime='2021-10-02T20:00:00+02:00'),\n",
       " Row(id='748076', consumptiongroup='household', endtime='2024-06-09T17:00:00+02:00', lastupdatedtime='2025-11-25T08:12:57+01:00', meteringpointcount='1103574', pricearea='NO1', quantitykwh='1472738.4', starttime='2024-06-09T16:00:00+02:00')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c = read_from_cassandra(table = 'consumption_per_group')\n",
    "save_as_csv(df_c, file_path= \"No_sync/C_Energy\")\n",
    "df_c.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production df: (871759, 4) \n",
      "   priceArea productionGroup                  startTime  quantityKwh\n",
      "0       NO1           other  2021-10-02T13:00:00+02:00        4.748\n",
      "1       NO2           hydro  2021-09-28T06:00:00+02:00  5560365.000\n",
      "Consumption df: (875400, 8) \n",
      "        id consumptiongroup                    endtime  \\\n",
      "0  264948         tertiary  2022-03-22T08:00:00+01:00   \n",
      "1    2975         tertiary  2021-01-01T05:00:00+01:00   \n",
      "\n",
      "             lastupdatedtime  meteringpointcount pricearea  quantitykwh  \\\n",
      "0  2025-03-31T08:07:25+02:00               55762       NO3    633554.75   \n",
      "1  2024-12-20T10:35:40+01:00               97386       NO1   1104153.60   \n",
      "\n",
      "                   starttime  \n",
      "0  2022-03-22T07:00:00+01:00  \n",
      "1  2021-01-01T04:00:00+01:00  \n"
     ]
    }
   ],
   "source": [
    "# Optional \n",
    "#df_p = pd.read_csv(\"No_sync/P_Energy.csv\")\n",
    "#df_c = pd.read_csv(\"No_sync/C_Energy.csv\")\n",
    "print(f\"Production df: {df_p.shape} \\n {df_p.head(2)}\")\n",
    "print(f\"Consumption df: { df_c.shape} \\n {df_c.head(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Inserting spark df to Mongo DB Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inserting_to_mongodb(df, collection = \"production_per_group\"):\n",
    "  # df should be spark df\n",
    "  client = MongoClient(uri)\n",
    "  db = client[\"indra\"]\n",
    "\n",
    "  # check if collection exists\n",
    "  if collection not in db.list_collection_names():\n",
    "      print(\"Collection does not exist. Creating...\")\n",
    "      db.create_collection(collection)\n",
    "      inserting_to_mongodb(df,collection=collection)\n",
    "  else:\n",
    "    df.write \\\n",
    "      .format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "      .option(\"spark.mongodb.output.uri\", uri) \\\n",
    "      .option(\"spark.mongodb.output.database\", \"indra\") \\\n",
    "      .option(\"spark.mongodb.output.collection\", collection) \\\n",
    "      .mode(\"overwrite\") \\\n",
    "      .save()\n",
    "\n",
    "    print(\"Success!\")\n",
    "\n",
    "  print(\"Data loading...\")\n",
    "\n",
    "  df_mongo = (\n",
    "          spark.read\n",
    "          .format(\"com.mongodb.spark.sql.DefaultSource\")  # for v10+ connector, this is correct\n",
    "          .option(\"spark.mongodb.input.uri\", uri)\n",
    "          .option(\"spark.mongodb.input.database\", \"indra\")\n",
    "          .option(\"spark.mongodb.input.collection\", collection)\n",
    "          .load()\n",
    "      )\n",
    "\n",
    "  print(\"Data Loaded.\")\n",
    "  df_mongo.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded.\n",
      "+--------------------------+---------+---------------+-----------+-------------------------+\n",
      "|_id                       |priceArea|productionGroup|quantityKwh|startTime                |\n",
      "+--------------------------+---------+---------------+-----------+-------------------------+\n",
      "|{69260eedd334ff502e7d5101}|NO5      |thermal        |76033.0    |2021-10-07T23:00:00+02:00|\n",
      "|{69260eedd334ff502e7d5102}|NO3      |other          |0.958      |2021-10-06T23:00:00+02:00|\n",
      "|{69260eedd334ff502e7d5103}|NO2      |solar          |4247.242   |2021-03-30T15:00:00+02:00|\n",
      "|{69260eedd334ff502e7d5104}|NO3      |hydro          |2643314.8  |2021-01-05T02:00:00+01:00|\n",
      "|{69260eedd334ff502e7d5105}|NO3      |other          |1.915      |2021-11-27T20:00:00+01:00|\n",
      "+--------------------------+---------+---------------+-----------+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inserting_to_mongodb(df_p, collection= \"production_per_group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/03 20:14:13 WARN DeprecatedConfigParameter: spark.cassandra.output.throughput_mb_per_sec is deprecated (DSE 6.0.0) and has been automatically replaced with parameter spark.cassandra.output.throughputMBPerSec. \n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n",
      "Data loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+----------------+-------------------------+------+-------------------------+------------------+---------+-----------+-------------------------+\n",
      "|_id                       |consumptiongroup|endtime                  |id    |lastupdatedtime          |meteringpointcount|pricearea|quantitykwh|starttime                |\n",
      "+--------------------------+----------------+-------------------------+------+-------------------------+------------------+---------+-----------+-------------------------+\n",
      "|{69308c0ab61887254137b2ea}|primary         |2024-09-30T16:00:00+02:00|818803|2025-11-26T10:55:16+01:00|2033              |NO5      |13000.991  |2024-09-30T15:00:00+02:00|\n",
      "|{69308c0ab61887254137b2eb}|secondary       |2024-02-14T15:00:00+01:00|687510|2025-11-24T02:54:24+01:00|6833              |NO4      |1174573.8  |2024-02-14T14:00:00+01:00|\n",
      "|{69308c0ab61887254137b2ec}|primary         |2022-09-22T05:00:00+02:00|380600|2025-03-31T18:09:28+02:00|2061              |NO5      |9885.398   |2022-09-22T04:00:00+02:00|\n",
      "|{69308c0ab61887254137b2ed}|cabin           |2022-06-23T13:00:00+02:00|316879|2025-03-31T05:59:40+02:00|58114             |NO3      |24797.41   |2022-06-23T12:00:00+02:00|\n",
      "|{69308c0ab61887254137b2ee}|primary         |2022-12-03T05:00:00+01:00|427792|2025-11-28T21:28:47+01:00|11570             |NO3      |83626.53   |2022-12-03T04:00:00+01:00|\n",
      "+--------------------------+----------------+-------------------------+------+-------------------------+------------------+---------+-----------+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "inserting_to_mongodb(df_c, collection= \"consumption_per_group\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
