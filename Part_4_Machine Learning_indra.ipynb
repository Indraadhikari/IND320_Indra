{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Work, Part 4 - Machine Learning\n",
    "## 1. Introduction\n",
    "This project involves analysing data with implementing machine learning model in a Jupyter Notebook and creating a multi-page online app with Streamlit, with all work and code shared on GitHub. AI tools (e.g., ChatGPT) were utilized during the project to clarify requirements and to gain a deeper understanding of the technologies used.\n",
    "\n",
    "- Task: Analysis of Norwegian electricity production (Elhub) and meteorological data (Open‑Meteo API).\n",
    "- Goal: automate data collection, perform time‑series decomposition, periodic analysis, and anomaly detection; then visualize results in a Jupyter Notebook and Streamlit dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Repository and App Links\n",
    "- GitHub: https://github.com/Indraadhikari/IND320_Indra\n",
    "- Streamlit app: https://ind320-k2r8aymxk9takanegm8e3y.streamlit.app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Project Overview\n",
    "### 3.1 AI Usage Description\n",
    "In this project, I used AI (ChatGPT) as a helpful assistant during development. It supported me in solving coding errors, generating code ideas, and improving my understanding of concepts. The AI explained topics such as STL decomposition, Discrete Cosine Transform (DCT) filtering, and Local Outlier Factor (LOF) anomaly detection, giving both theory and example code.\n",
    "\n",
    "I also used it to debug Python and Streamlit issues, like fixing empty DataFrames, using st.session_state, avoiding runtime errors, and organizing the multi-page layout. During implementation, I followed AI suggestions to clean up functions, set better parameter defaults, and make the visualizations easier to read.\n",
    "\n",
    "All AI outputs were carefully checked, tested, and modified to fit the project’s goals and my own coding style. Overall, the AI acted as a learning and support tool, helping me work faster and understand data analysis and software design more deeply.\n",
    "\n",
    "### 3.2 Project Log\n",
    "For the compulsory work, I began by defining representative cities for Norway’s five electricity price areas (NO1–NO5) and storing their latitude and longitude in a Pandas DataFrame. This mapping created the geographic foundation for the rest of the analyses. I then downloaded hourly electricity production data from the Elhub API for 2021, focusing on the *PRODUCTION_PER_GROUP_MBA_HOUR* dataset. The raw *JSON* responses were normalized into a clean DataFrame.\n",
    "\n",
    "Next, I replaced my earlier CSV‑based meteorological import with live calls to the Open‑Meteo API. For each selected price area, the application automatically queries the API using the corresponding city’s coordinates, returning hourly temperature, precipitation, and wind observations for 2019 in a Notebook file and 2021 for the Streamlit app. The fetched data are transformed into a tidy format in a Pandas DataFrame and cached for efficient reuse.\n",
    "\n",
    "Analytical development was divided into three main components, implemented and tested first in a Jupyter Notebook.\n",
    "- Seasonal‑Trend decomposition using LOESS (STL): using the *statsmodels.tsa.seasonal.STL* class, I decomposed the production time series into trend, seasonal, and residual components.\n",
    "- Spectrogram analysis: applying *scipy.signal.spectrogram*, I generated time–frequency plots to reveal changes in periodic behavior across the year.\n",
    "- Outlier and Anomaly detection: I implemented a robust Statistical Process Control (SPC) method using *Median ± k × MAD* boundaries on filtered temperature data and applied the Local Outlier Factor (LOF) algorithm from *scikit‑learn* to identify precipitation anomalies.\n",
    "Each analytical block was wrapped in a modular Python function with configurable parameters (area, group, window length, etc.) and tested interactively in the notebook before integration into the Streamlit app.\n",
    "\n",
    "I then updated the Streamlit dashboard to follow the new required page order. The global area selector was moved to the second page (named *Energy Production(4)* in the app), ensuring that all subsequent analyses depend on the user’s chosen region. Between existing pages, I added *new A (STL and Spectrogram(A))* and *new B (Outliers and Anomalies (B))* pages, each built with *st.tabs()* for navigation. Both pages render Matplotlib plots directly and display them. Communication between pages is managed through *st.session_state*, allowing the selected price area imported meteorological data and production data to persist throughout the session.\n",
    "\n",
    "The completed workflow demonstrates a full data pipeline: acquiring data dynamically via APIs, performing time‑series analysis, detecting anomalies, and presenting interactive results through a structured Streamlit interface and Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import pandas as pd\n",
    "import calendar\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fftpack import dct, idct\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from scipy.signal import spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Extraction and Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Connection Check for Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to Cassandra!\n",
      "Cluster name: Test Cluster\n",
      "Hosts: [<Host: ::1:9042 datacenter1>]\n"
     ]
    }
   ],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "try:\n",
    "    cluster = Cluster(['localhost'], port=9042)\n",
    "    session = cluster.connect()\n",
    "    print(\"✅ Connected to Cassandra!\")\n",
    "    print(\"Cluster name:\", cluster.metadata.cluster_name)\n",
    "    print(\"Hosts:\", cluster.metadata.all_hosts())\n",
    "    cluster.shutdown()\n",
    "except Exception as e:\n",
    "    print(\"❌ Connection failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Connection Check for MangoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "from pymongo.mongo_client import MongoClient\n",
    "\n",
    "c_file = '/Users/indra/Documents/Masters in Data Science/Data to Decision/IND320_Indra/No_sync/MongoDB.txt' #creadential file\n",
    "USR, PWD = open(c_file).read().splitlines()\n",
    "\n",
    "uri = \"mongodb+srv://\"+USR+\":\"+PWD+\"@cluster0.wmoqhtp.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "\n",
    "# Create a new client and connect to the server\n",
    "client = MongoClient(uri)\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Reading Data from  Elhub API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {    \n",
    "    \n",
    "}\n",
    "\n",
    "endpoint = \"https://api.elhub.no/energy-data/v0/\"\n",
    "entity = 'price-areas'\n",
    "dataset = \"PRODUCTION_PER_GROUP_MBA_HOUR\"\n",
    "#startdate = '2022-01-01T00:20:00%2B02:00'\n",
    "#enddate = '2024-12-31T23:59:59%2B02:00'\n",
    "year = [2022, 2023, 2024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(656700, 6)\n"
     ]
    }
   ],
   "source": [
    "import calendar\n",
    "import pandas as pd\n",
    "\n",
    "dates = []\n",
    "for i in year:\n",
    "    year = i\n",
    "    # accessing the data for a month at a time as Endpoint does not allow us to get for a whole year.\n",
    "    for month in range(1, 13):\n",
    "        # Get number of days in month\n",
    "        _, last_day = calendar.monthrange(year, month)\n",
    "        \n",
    "        # Format month and day properly (e.g. 01, 02, …)\n",
    "        startdate = f\"{year}-{month:02d}-01T00:20:00%2B02:00\"\n",
    "        enddate = f\"{year}-{month:02d}-{last_day:02d}T23:59:59%2B02:00\"\n",
    "        \n",
    "        dates.append((startdate, enddate))\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for startdate, enddate in dates:\n",
    "    #print(f\"Start: {start}   End: {end}\")\n",
    "    data = []\n",
    "    response = requests.get(f\"{endpoint}{entity}?dataset={dataset}&startDate={startdate}&endDate={enddate}\", headers=headers)\n",
    "    #print(response.status_code)\n",
    "    data = response.json()\n",
    "    #data['data'][1]['attributes']['productionPerGroupMbaHour']\n",
    "    for i in data['data']:\n",
    "        all_data.extend(i['attributes']['productionPerGroupMbaHour'])\n",
    "df = pd.DataFrame(all_data)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                     endTime            lastUpdatedTime priceArea  \\\n",
       " 0  2022-01-01T02:00:00+01:00  2025-02-01T18:02:57+01:00       NO1   \n",
       " 1  2022-01-01T03:00:00+01:00  2025-02-01T18:02:57+01:00       NO1   \n",
       " \n",
       "   productionGroup  quantityKwh                  startTime  \n",
       " 0           hydro    1246209.4  2022-01-01T01:00:00+01:00  \n",
       " 1           hydro    1271757.0  2022-01-01T02:00:00+01:00  ,\n",
       "                           endTime            lastUpdatedTime priceArea  \\\n",
       " 656698  2024-12-31T23:00:00+01:00  2025-03-30T18:39:27+02:00       NO5   \n",
       " 656699  2025-01-01T00:00:00+01:00  2025-03-30T18:39:27+02:00       NO5   \n",
       " \n",
       "        productionGroup  quantityKwh                  startTime  \n",
       " 656698            wind          0.0  2024-12-31T22:00:00+01:00  \n",
       " 656699            wind          0.0  2024-12-31T23:00:00+01:00  )"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2), df.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Creating Keyspace and Table in Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x1451bd210>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "#starting cassandra conection session\n",
    "\n",
    "cluster = Cluster(['localhost'], port=9042)\n",
    "session = cluster.connect()\n",
    "\n",
    "#making id columns for Premary Key for the table.\n",
    "if \"id\" not in df.columns:\n",
    "    df = df.reset_index().rename(columns={'index': 'id'})\n",
    "else:\n",
    "    pass\n",
    "df.columns\n",
    "\n",
    "columns = \", \".join([f\"{col} text\" for col in df.columns]) # type is text\n",
    "primary_key = df.columns[0]  # first column as primary key (id; index of the df)\n",
    "\n",
    "# Create a keyspace (database)\n",
    "session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS infindra\n",
    "    WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};\n",
    "\"\"\")\n",
    "\n",
    "#ALTER KEYSPACE infindra WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};\n",
    "\n",
    "session.set_keyspace('infindra')\n",
    "\n",
    "#Creating Tables\n",
    "create_query = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS production_per_group_4 (\n",
    "    {columns},\n",
    "    PRIMARY KEY ({primary_key})\n",
    ")\n",
    "\"\"\"\n",
    "session.execute(create_query)\n",
    "#session.execute(\"TRUNCATE TABLE production_per_group;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Inserting data to Cassandra using Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark version: 3.5.1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "os.environ['JAVA_HOME'] = \"/Library/Java/JavaVirtualMachines/microsoft-17.jdk/Contents/Home\"\n",
    "os.environ['PATH'] = os.path.join(os.environ['JAVA_HOME'], 'bin') + \":\" + os.environ['PATH']\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"CassandraReadTest\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.jars.repositories\",\n",
    "            \"https://repos.spark-packages.org,https://oss.sonatype.org/content/repositories/releases/\")\n",
    "    .config(\"spark.jars.packages\",\n",
    "            \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.1,\"\n",
    "            \"org.mongodb.spark:mongo-spark-connector_2.12:3.5.1\")\n",
    "    .config(\"spark.cassandra.connection.host\", \"localhost\")\n",
    "    .config(\"spark.cassandra.connection.port\", \"9042\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(f\"✅ Spark version: {spark.version}\")\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/25 18:29:34 WARN TaskSetManager: Stage 0 contains a task of very large size (9102 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/25 18:29:39 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 0 (TID 0): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+---------+---------------+-----------+--------------------+\n",
      "| id|             endtime|     lastupdatedtime|pricearea|productiongroup|quantitykwh|           starttime|\n",
      "+---+--------------------+--------------------+---------+---------------+-----------+--------------------+\n",
      "|  0|2022-01-01T02:00:...|2025-02-01T18:02:...|      NO1|          hydro|  1246209.4|2022-01-01T01:00:...|\n",
      "|  1|2022-01-01T03:00:...|2025-02-01T18:02:...|      NO1|          hydro|  1271757.0|2022-01-01T02:00:...|\n",
      "+---+--------------------+--------------------+---------+---------------+-----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "spark_df = spark_df.toDF(*[c.lower() for c in spark_df.columns]) #changing the colomns name in lower case to match casandra table\n",
    "#spark_df.printSchema()\n",
    "# Show the data\n",
    "spark_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize Cassandra write settings\n",
    "spark.conf.set(\"spark.cassandra.output.concurrent.writes\", \"5\")\n",
    "spark.conf.set(\"spark.cassandra.output.throughput_mb_per_sec\", \"200\")\n",
    "spark.conf.set(\"spark.cassandra.output.batch.size.rows\", \"1000\")\n",
    "\n",
    "# Write DataFrame to Cassandra\n",
    "# keyspace='infindra' and table='production_per_group' exist in Cassandra\n",
    "spark_df.write \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .option(\"keyspace\", \"infindra\") \\\n",
    "    .option(\"table\", \"production_per_group_4\") \\\n",
    "    .option(\"confirm.truncate\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"Data successfully written to Cassandra!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Reading Data from Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(priceArea='NO1', productionGroup='thermal', startTime='2023-03-15T12:00:00+01:00', quantityKwh='42119.51'),\n",
       " Row(priceArea='NO3', productionGroup='solar', startTime='2023-10-04T08:00:00+02:00', quantityKwh='84.075'),\n",
       " Row(priceArea='NO4', productionGroup='thermal', startTime='2023-08-05T07:00:00+02:00', quantityKwh='225001.0'),\n",
       " Row(priceArea='NO1', productionGroup='solar', startTime='2024-03-01T03:00:00+01:00', quantityKwh='308.645'),\n",
       " Row(priceArea='NO5', productionGroup='other', startTime='2022-10-02T20:00:00+02:00', quantityKwh='0.003')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataframe for Elhub API´s PRODUCTION_PER_GROUP_MBA_HOUR data for 2021\n",
    "df_c = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .option(\"keyspace\", \"infindra\") \\\n",
    "    .option(\"table\", \"production_per_group\") \\\n",
    "    .load()\n",
    "\n",
    "df_c_new = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .option(\"keyspace\", \"infindra\") \\\n",
    "    .option(\"table\", \"production_per_group_4\") \\\n",
    "    .load()\n",
    "\n",
    "df_merge = df_c.union(df_c_new)\n",
    "\n",
    "selected_df = df_merge.select(\"priceArea\", \"productionGroup\", \"startTime\", \"quantityKwh\")\n",
    "\n",
    "#selected_df.show()\n",
    "selected_df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Inserting spark df to Mongo DB Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded.\n",
      "+--------------------------+---------+---------------+-----------+-------------------------+\n",
      "|_id                       |priceArea|productionGroup|quantityKwh|startTime                |\n",
      "+--------------------------+---------+---------------+-----------+-------------------------+\n",
      "|{69260eedd334ff502e7d5101}|NO5      |thermal        |76033.0    |2021-10-07T23:00:00+02:00|\n",
      "|{69260eedd334ff502e7d5102}|NO3      |other          |0.958      |2021-10-06T23:00:00+02:00|\n",
      "|{69260eedd334ff502e7d5103}|NO2      |solar          |4247.242   |2021-03-30T15:00:00+02:00|\n",
      "|{69260eedd334ff502e7d5104}|NO3      |hydro          |2643314.8  |2021-01-05T02:00:00+01:00|\n",
      "|{69260eedd334ff502e7d5105}|NO3      |other          |1.915      |2021-11-27T20:00:00+01:00|\n",
      "+--------------------------+---------+---------------+-----------+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "selected_df.write \\\n",
    "  .format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "  .option(\"spark.mongodb.output.uri\", uri) \\\n",
    "  .option(\"spark.mongodb.output.database\", \"indra\") \\\n",
    "  .option(\"spark.mongodb.output.collection\", \"production_per_group\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save()\n",
    "\n",
    "print(\"Success!\")\n",
    "\n",
    "df_mongo = (\n",
    "        spark.read\n",
    "        .format(\"com.mongodb.spark.sql.DefaultSource\")  # for v10+ connector, this is correct\n",
    "        .option(\"spark.mongodb.input.uri\", uri)\n",
    "        .option(\"spark.mongodb.input.database\", \"indra\")\n",
    "        .option(\"spark.mongodb.input.collection\", \"production_per_group\")\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "print(\"Data Loaded.\")\n",
    "df_mongo.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(_id=Row(oid='69260fcdd334ff502e896c2b'), priceArea='NO1', productionGroup='solar', quantityKwh='325.507', startTime='2024-12-31T23:00:00+01:00'),\n",
       " Row(_id=Row(oid='69260fd9d334ff502e89e2cf'), priceArea='NO4', productionGroup='hydro', quantityKwh='2677024.0', startTime='2024-12-31T23:00:00+01:00'),\n",
       " Row(_id=Row(oid='69260fdbd334ff502e89f407'), priceArea='NO3', productionGroup='other', quantityKwh='25.577', startTime='2024-12-31T23:00:00+01:00'),\n",
       " Row(_id=Row(oid='69260febd334ff502e8a4acb'), priceArea='NO4', productionGroup='solar', quantityKwh='0.0', startTime='2024-12-31T23:00:00+01:00'),\n",
       " Row(_id=Row(oid='69260fffd334ff502e8a7605'), priceArea='NO3', productionGroup='solar', quantityKwh='66.082', startTime='2024-12-31T23:00:00+01:00')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mongo.orderBy(\"startTime\").tail(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
