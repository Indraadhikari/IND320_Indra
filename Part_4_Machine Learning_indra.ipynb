{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Work, Part 4 - Machine Learning\n",
    "## 1. Introduction\n",
    "This project involves analysing data with implementing machine learning model in a Jupyter Notebook and creating a multi-page online app with Streamlit, with all work and code shared on GitHub. AI tools (e.g., ChatGPT) were utilized during the project to clarify requirements and to gain a deeper understanding of the technologies used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Repository and App Links\n",
    "- GitHub: https://github.com/Indraadhikari/IND320_Indra\n",
    "- Streamlit app: https://ind320-k2r8aymxk9takanegm8e3y.streamlit.app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Project Overview\n",
    "### 3.1 AI Usage Description\n",
    "In this part of the project, I leveraged AI (ChatGPT/Claude) as a development assistant. AI helped in designing a multi-page Streamlit application, debugging Python issues with time series and geospatial data, and improving code structure. Specifically, AI assisted in:\n",
    "\n",
    "- Handling misaligned DataFrames and cleaning missing values in energy and meteorological data to avoid SARIMAX errors\n",
    "- Implementing GeoJSON processing for interactive maps with coordinate selection and session state management\n",
    "- Translating Tabler (2003) snow drift calculations and creating 16-sector wind rose diagrams\n",
    "- Structuring sliding window correlation analysis with configurable lag parameters\n",
    "- Debugging statsmodels errors related to exogenous variables and timestamp arithmetic\n",
    "- Implementing caching strategies and Plotly visualizations with confidence intervals\n",
    "\n",
    "All AI outputs were validated and adjusted to ensure correctness and reproducibility.\n",
    "\n",
    "### 3.2 Project Log\n",
    "Updated the global selection mechanism using `st.session_state` to store selected price area (NO1-NO5) and coordinates, ensuring consistency across all pages.\n",
    "\n",
    "**Interactive Map:** Created GeoJSON-based choropleth visualization of Norwegian price areas. Users can click to select areas and coordinates, with mean energy production calculated by time interval (7-90 days) and energy group (hydro, wind, solar, etc.). Selected areas are highlighted with red outlines and coordinates marked with stars.\n",
    "\n",
    "**Snow Drift Analysis:** Implemented Tabler (2003) method with seasons defined as July 1 to June 30. Fetched hourly meteorological data from Open-Meteo API (temperature, precipitation, wind speed, direction). Calculated potential and actual snow transport (Qupot, Qt) and created wind rose diagrams showing directional distribution. Uses coordinates from map selection.\n",
    "\n",
    "**Correlation Analysis:** Built sliding window correlation with selectable lag (-168 to +168 hours) between meteorological variables and energy production. Aligned hourly weather data with energy time series, detected extreme events (>2σ), and created three-panel visualizations showing correlation changes, weather patterns, and production levels over time.\n",
    "\n",
    "**SARIMAX Forecasting:** Implemented time series forecasting with full parameter control (p,d,q)(P,D,Q,s). Users select energy group, training period, and forecast horizon (24-720 hours). Optional exogenous variables use hourly seasonal patterns for future periods. Results display historical data, forecasts, and 95% confidence intervals with model metrics (AIC, BIC).\n",
    "\n",
    "**Data Pipeline:** Elhub API → MongoDB → Streamlit (energy data); Open-Meteo API → cleaning/alignment → analysis (weather data); NVE API → GeoPandas → Plotly (GeoJSON).\n",
    "\n",
    "The completed workflow demonstrates a full data pipeline: acquiring data dynamically via APIs, performing time‑series analysis, detecting anomalies, and presenting interactive results through a structured Streamlit interface and Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import pandas as pd\n",
    "import calendar\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fftpack import dct, idct\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from scipy.signal import spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Extraction and Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Connection Check for Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to Cassandra!\n",
      "Cluster name: Test Cluster\n",
      "Hosts: [<Host: 127.0.0.1:9042 datacenter1>]\n"
     ]
    }
   ],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "try:\n",
    "    cluster = Cluster(['localhost'], port=9042)\n",
    "    session = cluster.connect()\n",
    "    print(\"✅ Connected to Cassandra!\")\n",
    "    print(\"Cluster name:\", cluster.metadata.cluster_name)\n",
    "    print(\"Hosts:\", cluster.metadata.all_hosts())\n",
    "    cluster.shutdown()\n",
    "except Exception as e:\n",
    "    print(\"❌ Connection failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Connection Check for MangoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "from pymongo.mongo_client import MongoClient\n",
    "\n",
    "c_file = '/Users/indra/Documents/Masters in Data Science/Data to Decision/IND320_Indra/No_sync/MongoDB.txt' #creadential file\n",
    "USR, PWD = open(c_file).read().splitlines()\n",
    "\n",
    "uri = \"mongodb+srv://\"+USR+\":\"+PWD+\"@cluster0.wmoqhtp.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "\n",
    "# Create a new client and connect to the server\n",
    "client = MongoClient(uri)\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Reading Data from  Elhub API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {    \n",
    "    \n",
    "}\n",
    "\n",
    "endpoint = \"https://api.elhub.no/energy-data/v0/\"\n",
    "entity = 'price-areas'\n",
    "dataset = \"PRODUCTION_PER_GROUP_MBA_HOUR\"\n",
    "#startdate = '2022-01-01T00:20:00%2B02:00'\n",
    "#enddate = '2024-12-31T23:59:59%2B02:00'\n",
    "year = [2022, 2023, 2024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(656700, 6)\n"
     ]
    }
   ],
   "source": [
    "import calendar\n",
    "import pandas as pd\n",
    "\n",
    "dates = []\n",
    "for i in year:\n",
    "    year = i\n",
    "    # accessing the data for a month at a time as Endpoint does not allow us to get for a whole year.\n",
    "    for month in range(1, 13):\n",
    "        # Get number of days in month\n",
    "        _, last_day = calendar.monthrange(year, month)\n",
    "        \n",
    "        # Format month and day properly (e.g. 01, 02, …)\n",
    "        startdate = f\"{year}-{month:02d}-01T00:20:00%2B02:00\"\n",
    "        enddate = f\"{year}-{month:02d}-{last_day:02d}T23:59:59%2B02:00\"\n",
    "        \n",
    "        dates.append((startdate, enddate))\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for startdate, enddate in dates:\n",
    "    #print(f\"Start: {start}   End: {end}\")\n",
    "    data = []\n",
    "    response = requests.get(f\"{endpoint}{entity}?dataset={dataset}&startDate={startdate}&endDate={enddate}\", headers=headers)\n",
    "    #print(response.status_code)\n",
    "    data = response.json()\n",
    "    #data['data'][1]['attributes']['productionPerGroupMbaHour']\n",
    "    for i in data['data']:\n",
    "        all_data.extend(i['attributes']['productionPerGroupMbaHour'])\n",
    "df = pd.DataFrame(all_data)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                     endTime            lastUpdatedTime priceArea  \\\n",
       " 0  2022-01-01T02:00:00+01:00  2025-02-01T18:02:57+01:00       NO1   \n",
       " 1  2022-01-01T03:00:00+01:00  2025-02-01T18:02:57+01:00       NO1   \n",
       " \n",
       "   productionGroup  quantityKwh                  startTime  \n",
       " 0           hydro    1246209.4  2022-01-01T01:00:00+01:00  \n",
       " 1           hydro    1271757.0  2022-01-01T02:00:00+01:00  ,\n",
       "                           endTime            lastUpdatedTime priceArea  \\\n",
       " 656698  2024-12-31T23:00:00+01:00  2025-03-30T18:39:27+02:00       NO5   \n",
       " 656699  2025-01-01T00:00:00+01:00  2025-03-30T18:39:27+02:00       NO5   \n",
       " \n",
       "        productionGroup  quantityKwh                  startTime  \n",
       " 656698            wind          0.0  2024-12-31T22:00:00+01:00  \n",
       " 656699            wind          0.0  2024-12-31T23:00:00+01:00  )"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2), df.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Creating Keyspace and Table in Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x1451bd210>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "#starting cassandra conection session\n",
    "\n",
    "cluster = Cluster(['localhost'], port=9042)\n",
    "session = cluster.connect()\n",
    "\n",
    "#making id columns for Premary Key for the table.\n",
    "if \"id\" not in df.columns:\n",
    "    df = df.reset_index().rename(columns={'index': 'id'})\n",
    "else:\n",
    "    pass\n",
    "df.columns\n",
    "\n",
    "columns = \", \".join([f\"{col} text\" for col in df.columns]) # type is text\n",
    "primary_key = df.columns[0]  # first column as primary key (id; index of the df)\n",
    "\n",
    "# Create a keyspace (database)\n",
    "session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS infindra\n",
    "    WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};\n",
    "\"\"\")\n",
    "\n",
    "#ALTER KEYSPACE infindra WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};\n",
    "\n",
    "session.set_keyspace('infindra')\n",
    "\n",
    "#Creating Tables\n",
    "create_query = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS production_per_group_4 (\n",
    "    {columns},\n",
    "    PRIMARY KEY ({primary_key})\n",
    ")\n",
    "\"\"\"\n",
    "session.execute(create_query)\n",
    "#session.execute(\"TRUNCATE TABLE production_per_group;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Inserting data to Cassandra using Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/28 13:25:00 WARN Utils: Your hostname, MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.9 instead (on interface en0)\n",
      "25/11/28 13:25:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "https://repos.spark-packages.org added as a remote repository with the name: repo-1\n",
      "https://oss.sonatype.org/content/repositories/releases/ added as a remote repository with the name: repo-2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/indra/Library/Python/3.11/lib/python/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/indra/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/indra/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-2c30f9d3-bf9e-4017-81b9-8f40741b1375;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.5.1 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.1 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 in central\n",
      "\tfound org.apache.cassandra#java-driver-core-shaded;4.18.1 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.1 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound org.apache.cassandra#java-driver-mapper-runtime;4.18.1 in central\n",
      "\tfound org.apache.cassandra#java-driver-query-builder;4.18.1 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.19 in central\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.2 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 725ms :: artifacts dl 77ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.1 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.1 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.5.1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.cassandra#java-driver-core-shaded;4.18.1 from central in [default]\n",
      "\torg.apache.cassandra#java-driver-mapper-runtime;4.18.1 from central in [default]\n",
      "\torg.apache.cassandra#java-driver-query-builder;4.18.1 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.2 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.19 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.11.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   20  |   0   |   0   |   0   ||   20  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-2c30f9d3-bf9e-4017-81b9-8f40741b1375\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 20 already retrieved (0kB/30ms)\n",
      "25/11/28 13:25:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/28 13:25:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark version: 3.5.1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "os.environ['JAVA_HOME'] = \"/Library/Java/JavaVirtualMachines/microsoft-17.jdk/Contents/Home\"\n",
    "os.environ['PATH'] = os.path.join(os.environ['JAVA_HOME'], 'bin') + \":\" + os.environ['PATH']\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"CassandraReadTest\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.jars.repositories\",\n",
    "            \"https://repos.spark-packages.org,https://oss.sonatype.org/content/repositories/releases/\")\n",
    "    .config(\"spark.jars.packages\",\n",
    "            \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.1,\"\n",
    "            \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.2\")\n",
    "    .config(\"spark.cassandra.connection.host\", \"localhost\")\n",
    "    .config(\"spark.cassandra.connection.port\", \"9042\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(f\"✅ Spark version: {spark.version}\")\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/25 18:29:34 WARN TaskSetManager: Stage 0 contains a task of very large size (9102 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/25 18:29:39 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 0 (TID 0): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+---------+---------------+-----------+--------------------+\n",
      "| id|             endtime|     lastupdatedtime|pricearea|productiongroup|quantitykwh|           starttime|\n",
      "+---+--------------------+--------------------+---------+---------------+-----------+--------------------+\n",
      "|  0|2022-01-01T02:00:...|2025-02-01T18:02:...|      NO1|          hydro|  1246209.4|2022-01-01T01:00:...|\n",
      "|  1|2022-01-01T03:00:...|2025-02-01T18:02:...|      NO1|          hydro|  1271757.0|2022-01-01T02:00:...|\n",
      "+---+--------------------+--------------------+---------+---------------+-----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "spark_df = spark_df.toDF(*[c.lower() for c in spark_df.columns]) #changing the colomns name in lower case to match casandra table\n",
    "#spark_df.printSchema()\n",
    "# Show the data\n",
    "spark_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize Cassandra write settings\n",
    "spark.conf.set(\"spark.cassandra.output.concurrent.writes\", \"5\")\n",
    "spark.conf.set(\"spark.cassandra.output.throughput_mb_per_sec\", \"200\")\n",
    "spark.conf.set(\"spark.cassandra.output.batch.size.rows\", \"1000\")\n",
    "\n",
    "# Write DataFrame to Cassandra\n",
    "# keyspace='infindra' and table='production_per_group' exist in Cassandra\n",
    "spark_df.write \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .option(\"keyspace\", \"infindra\") \\\n",
    "    .option(\"table\", \"production_per_group_4\") \\\n",
    "    .option(\"confirm.truncate\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"Data successfully written to Cassandra!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Reading Data from Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(priceArea='NO2', productionGroup='solar', startTime='2024-12-26T19:00:00+01:00', quantityKwh='72.243'),\n",
       " Row(priceArea='NO4', productionGroup='hydro', startTime='2023-04-25T09:00:00+02:00', quantityKwh='3419812.2'),\n",
       " Row(priceArea='NO4', productionGroup='wind', startTime='2024-07-29T15:00:00+02:00', quantityKwh='94313.76'),\n",
       " Row(priceArea='NO3', productionGroup='hydro', startTime='2023-02-03T11:00:00+01:00', quantityKwh='2922081.5'),\n",
       " Row(priceArea='NO2', productionGroup='thermal', startTime='2022-06-14T00:00:00+02:00', quantityKwh='24708.13')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataframe for Elhub API´s PRODUCTION_PER_GROUP_MBA_HOUR data for 2021\n",
    "df_c = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .option(\"keyspace\", \"infindra\") \\\n",
    "    .option(\"table\", \"production_per_group\") \\\n",
    "    .load()\n",
    "\n",
    "df_c_new = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .option(\"keyspace\", \"infindra\") \\\n",
    "    .option(\"table\", \"production_per_group_4\") \\\n",
    "    .load()\n",
    "\n",
    "df_merge = df_c.union(df_c_new)\n",
    "\n",
    "selected_df = df_merge.select(\"priceArea\", \"productionGroup\", \"startTime\", \"quantityKwh\")\n",
    "\n",
    "selected_df.coalesce(1) \\\n",
    "  .write \\\n",
    "  .option(\"header\", True) \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .csv(\"output/\")\n",
    "# selected_df.show()\n",
    "selected_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(871759, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"No_sync/P_Energy.csv\")\n",
    "#df1 = pd.read_csv(\"energy.csv\")\n",
    "df1.head()\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Inserting spark df to Mongo DB Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded.\n",
      "+--------------------------+---------+---------------+-----------+-------------------------+\n",
      "|_id                       |priceArea|productionGroup|quantityKwh|startTime                |\n",
      "+--------------------------+---------+---------------+-----------+-------------------------+\n",
      "|{69260eedd334ff502e7d5101}|NO5      |thermal        |76033.0    |2021-10-07T23:00:00+02:00|\n",
      "|{69260eedd334ff502e7d5102}|NO3      |other          |0.958      |2021-10-06T23:00:00+02:00|\n",
      "|{69260eedd334ff502e7d5103}|NO2      |solar          |4247.242   |2021-03-30T15:00:00+02:00|\n",
      "|{69260eedd334ff502e7d5104}|NO3      |hydro          |2643314.8  |2021-01-05T02:00:00+01:00|\n",
      "|{69260eedd334ff502e7d5105}|NO3      |other          |1.915      |2021-11-27T20:00:00+01:00|\n",
      "+--------------------------+---------+---------------+-----------+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "selected_df.write \\\n",
    "  .format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "  .option(\"spark.mongodb.output.uri\", uri) \\\n",
    "  .option(\"spark.mongodb.output.database\", \"indra\") \\\n",
    "  .option(\"spark.mongodb.output.collection\", \"production_per_group\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save()\n",
    "\n",
    "print(\"Success!\")\n",
    "\n",
    "df_mongo = (\n",
    "        spark.read\n",
    "        .format(\"com.mongodb.spark.sql.DefaultSource\")  # for v10+ connector, this is correct\n",
    "        .option(\"spark.mongodb.input.uri\", uri)\n",
    "        .option(\"spark.mongodb.input.database\", \"indra\")\n",
    "        .option(\"spark.mongodb.input.collection\", \"production_per_group\")\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "print(\"Data Loaded.\")\n",
    "df_mongo.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(_id=Row(oid='69260fcdd334ff502e896c2b'), priceArea='NO1', productionGroup='solar', quantityKwh='325.507', startTime='2024-12-31T23:00:00+01:00'),\n",
       " Row(_id=Row(oid='69260fd9d334ff502e89e2cf'), priceArea='NO4', productionGroup='hydro', quantityKwh='2677024.0', startTime='2024-12-31T23:00:00+01:00'),\n",
       " Row(_id=Row(oid='69260fdbd334ff502e89f407'), priceArea='NO3', productionGroup='other', quantityKwh='25.577', startTime='2024-12-31T23:00:00+01:00'),\n",
       " Row(_id=Row(oid='69260febd334ff502e8a4acb'), priceArea='NO4', productionGroup='solar', quantityKwh='0.0', startTime='2024-12-31T23:00:00+01:00'),\n",
       " Row(_id=Row(oid='69260fffd334ff502e8a7605'), priceArea='NO3', productionGroup='solar', quantityKwh='66.082', startTime='2024-12-31T23:00:00+01:00')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mongo.orderBy(\"startTime\").tail(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
